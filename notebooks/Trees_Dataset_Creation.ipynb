{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48M9Mm3xd_1e"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the Grounding DINO repository\n",
        "!git clone https://github.com/IDEA-Research/GroundingDINO.git\n",
        "%cd GroundingDINO/\n",
        "\n",
        "# Install dependencies\n",
        "!pip install -e .\n",
        "\n",
        "# Download pre-trained weights\n",
        "!mkdir weights\n",
        "!wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth -P weights/"
      ],
      "metadata": {
        "id": "3qAVHEN6eIXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change back to the main directory where you want Grounded-SAM-2 to live\n",
        "%cd /content/\n",
        "\n",
        "# Clone the Grounded SAM 2 repository\n",
        "!git clone https://github.com/IDEA-Research/Grounded-SAM-2.git\n",
        "%cd Grounded-SAM-2\n",
        "\n",
        "# Initialize and update submodules (important for dependencies)\n",
        "!git submodule update --init --recursive"
      ],
      "metadata": {
        "id": "nyjiSXY_eK4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change to the /content directory (ensure we start from a known location)\n",
        "%cd /content/\n",
        "\n",
        "# Verify the current working directory\n",
        "!pwd\n",
        "\n",
        "# Clone the Grounded SAM 2 repository (if it doesn't exist)\n",
        "import os\n",
        "if not os.path.exists(\"Grounded-SAM-2\"):\n",
        "    !git clone https://github.com/IDEA-Research/Grounded-SAM-2.git\n",
        "\n",
        "# Change directory into Grounded-SAM-2\n",
        "%cd Grounded-SAM-2\n",
        "\n",
        "# Verify the current working directory *again*\n",
        "!pwd\n",
        "\n",
        "# Initialize and update submodules (very important)\n",
        "!git submodule update --init --recursive\n",
        "\n",
        "# List the contents of the sam2 directory\n",
        "!ls sam2\n",
        "\n",
        "# Search for setup.py and pyproject.toml within the entire Grounded-SAM-2 directory\n",
        "!find . -name \"setup.py\"\n",
        "!find . -name \"pyproject.toml\"\n",
        "\n",
        "# Install SAM 2 (Segment Anything 2) - VERY CAREFULLY\n",
        "%cd sam2  # Change directory *into* the sam2 directory\n",
        "!pwd  # Verify we're in the sam2 directory\n",
        "!pip install -e .  # Install from the *current* directory\n",
        "\n",
        "# Change back to the Grounded-SAM-2 directory\n",
        "%cd .."
      ],
      "metadata": {
        "id": "aX0y4lwieNIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change to the /content directory\n",
        "%cd /content/\n",
        "\n",
        "# Remove the existing Grounded-SAM-2 directory (CAREFUL!)\n",
        "!rm -rf Grounded-SAM-2\n",
        "\n",
        "# Clone the Grounded SAM 2 repository (FORCE A RE-CLONE)\n",
        "!git clone https://github.com/IDEA-Research/Grounded-SAM-2.git\n",
        "\n",
        "# Change directory into Grounded-SAM-2\n",
        "%cd Grounded-SAM-2\n",
        "\n",
        "# List all files in the Grounded-SAM-2 directory to CONFIRM requirements.txt\n",
        "!ls -l\n",
        "\n",
        "# Verify that requirements.txt exists\n",
        "import os\n",
        "if not os.path.exists(\"requirements.txt\"):\n",
        "    print(\"ERROR: requirements.txt is missing!\")\n",
        "    print(\"Please ensure you have cloned the correct repository.\")\n",
        "    exit()  # Stop execution\n",
        "\n",
        "# Initialize and update submodules\n",
        "!git submodule update --init --recursive\n",
        "\n",
        "# Download Grounding DINO checkpoints\n",
        "%cd gdino_checkpoints\n",
        "!bash download_ckpts.sh\n",
        "%cd ..\n",
        "\n",
        "# Install dependencies from requirements.txt\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "DM3ePNpEeQoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Grounded-SAM-2/checkpoints\n",
        "!bash download_ckpts.sh\n",
        "%cd .."
      ],
      "metadata": {
        "id": "4kzIeucDeYo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Grounded-SAM-2/\n",
        "\n",
        "!pip install torch torchvision torchaudio  # PyTorch and related libraries\n",
        "!pip install opencv-python  # OpenCV for image/video processing\n",
        "!pip install transformers  # Hugging Face Transformers\n",
        "!pip install diffusers  # Hugging Face Diffusers (may not be needed, but often used)\n",
        "!pip install accelerate  # Hugging Face Accelerate (for distributed training)\n",
        "!pip install supervision  # Supervision library (mentioned in the README)\n",
        "!pip install Pillow  # Python Imaging Library\n",
        "!pip install matplotlib  # Plotting library\n",
        "!pip install scikit-image  # Image processing tools\n",
        "!pip install pycocotools # For COCO dataset format\n",
        "!pip install timm # PyTorch Image Models\n",
        "!pip install huggingface_hub # Access Hugging Face models\n",
        "!pip install sahi #Slicing Aided Hyper Inference\n",
        "!pip install dds-cloudapi-sdk --upgrade"
      ],
      "metadata": {
        "id": "fVOil_20ebDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install setuptools wheel\n",
        "!pip install --upgrade pip\n",
        "!pip install hydra-core\n",
        "!pip install addict\n",
        "!pip install yapf\n",
        "!pip install iopath\n",
        "!pip install requests"
      ],
      "metadata": {
        "id": "wkkKCAyiecHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify CUDA installation\n",
        "!nvcc --version\n",
        "\n",
        "# Rebuild the MSDeformAttn extension\n",
        "%cd /content/Grounded-SAM-2/grounding_dino\n",
        "!python setup.py build_ext --inplace\n",
        "%cd ..\n",
        "\n",
        "# (Optional) Force CPU execution (if CUDA is not available or not working)\n",
        "# Modify grounded_sam2_tracking_demo_custom_video_input_gd1.0_local_model.py to set DEVICE = torch.device(\"cpu\")\n",
        "\n",
        "# Run the tracking demo\n",
        "# %cd /content/Grounded-SAM-2/\n",
        "# !python grounded_sam2_tracking_demo_custom_video_input_gd1.0_local_model.py"
      ],
      "metadata": {
        "id": "AvRjFf-uegFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "video_path = \"/content/Untitled design (1).mp4\"\n",
        "output_dir = \"frames/\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "idx = 0\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    cv2.imwrite(os.path.join(output_dir, f\"{idx:05d}.jpg\"), frame)\n",
        "    idx += 1\n",
        "cap.release()\n",
        "\n",
        "\n",
        "video_dir = \"/frames\"  # path to your extracted frames\n",
        "output_dir = \"./outputs\"  # optional, can change\n",
        "output_video_path = \"./outputs/output.mp4\"  # optional\n",
        "text = \"trees, plants, bushes.\"  # your detection query"
      ],
      "metadata": {
        "id": "w_s96Q0QejqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import supervision as sv\n",
        "from PIL import Image\n",
        "from sam2.build_sam import build_sam2_video_predictor, build_sam2\n",
        "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
        "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
        "from utils.track_utils import sample_points_from_masks\n",
        "from utils.video_utils import create_video_from_images\n",
        "from utils.common_utils import CommonUtils\n",
        "from utils.mask_dictionary_model import MaskDictionaryModel, ObjectInfo\n",
        "import json\n",
        "import copy\n",
        "import io\n",
        "import requests\n",
        "import time\n",
        "import re\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "\n",
        "class VideoTimestampGenerator:\n",
        "    \"\"\"Helper class for generating millisecond precision timestamps based on video start time\"\"\"\n",
        "\n",
        "    def __init__(self, video_start_time=None, fps=30.0):\n",
        "        \"\"\"\n",
        "        Initialize with video start time\n",
        "\n",
        "        Args:\n",
        "            video_start_time: Can be:\n",
        "                - datetime object\n",
        "                - ISO string like \"2025-07-07T10:30:00.123\"\n",
        "                - Unix timestamp in seconds\n",
        "                - Unix timestamp in milliseconds (13 digits)\n",
        "                - Video filename (will extract timestamp)\n",
        "                - None (will prompt for input)\n",
        "            fps: Frames per second of the video\n",
        "        \"\"\"\n",
        "        self.fps = fps\n",
        "\n",
        "        if video_start_time is None:\n",
        "            print(\"Please provide the video start time:\")\n",
        "            print(\"Examples:\")\n",
        "            print(\"  - '2025-07-07T10:30:00.123'\")\n",
        "            print(\"  - '1735724709230' (milliseconds)\")\n",
        "            print(\"  - '1735724709' (seconds)\")\n",
        "            print(\"  - Video filename like '1735724709230.mp4'\")\n",
        "            user_input = input(\"Video start time: \").strip()\n",
        "            video_start_time = user_input\n",
        "\n",
        "        self.video_start_time_ms = self._parse_start_time(video_start_time)\n",
        "\n",
        "        # Convert to datetime for display\n",
        "        self.video_start_datetime = datetime.fromtimestamp(self.video_start_time_ms / 1000.0)\n",
        "        print(f\"📅 Video start time set to: {self.video_start_datetime.isoformat()}\")\n",
        "        print(f\"📅 Video start timestamp: {self.video_start_time_ms} ms\")\n",
        "\n",
        "    def _parse_start_time(self, start_time):\n",
        "        \"\"\"Parse various start time formats into milliseconds since epoch\"\"\"\n",
        "\n",
        "        if isinstance(start_time, datetime):\n",
        "            return int(start_time.timestamp() * 1000)\n",
        "\n",
        "        elif isinstance(start_time, (int, float)):\n",
        "            # Check if it's already in milliseconds (13 digits) or seconds (10 digits)\n",
        "            if start_time > 1e12:  # Likely milliseconds (13+ digits)\n",
        "                return int(start_time)\n",
        "            else:  # Likely seconds since epoch (10 digits)\n",
        "                return int(start_time * 1000)\n",
        "\n",
        "        elif isinstance(start_time, str):\n",
        "            # First try to extract timestamp from filename\n",
        "            timestamp_from_filename = self._extract_timestamp_from_filename(start_time)\n",
        "            if timestamp_from_filename:\n",
        "                return timestamp_from_filename\n",
        "\n",
        "            # Try parsing as datetime string\n",
        "            try:\n",
        "                # Try parsing ISO format\n",
        "                if 'T' in start_time:\n",
        "                    dt = datetime.fromisoformat(start_time.replace('Z', '+00:00'))\n",
        "                else:\n",
        "                    # Try parsing common formats\n",
        "                    for fmt in ['%Y-%m-%d %H:%M:%S.%f', '%Y-%m-%d %H:%M:%S', '%Y-%m-%d']:\n",
        "                        try:\n",
        "                            dt = datetime.strptime(start_time, fmt)\n",
        "                            break\n",
        "                        except ValueError:\n",
        "                            continue\n",
        "                    else:\n",
        "                        # Try parsing as pure number string\n",
        "                        try:\n",
        "                            timestamp_num = int(start_time)\n",
        "                            if timestamp_num > 1e12:  # Milliseconds\n",
        "                                return timestamp_num\n",
        "                            else:  # Seconds\n",
        "                                return timestamp_num * 1000\n",
        "                        except ValueError:\n",
        "                            raise ValueError(f\"Could not parse datetime string: {start_time}\")\n",
        "\n",
        "                return int(dt.timestamp() * 1000)\n",
        "\n",
        "            except ValueError as e:\n",
        "                raise ValueError(f\"Invalid start time format: {start_time}. Error: {e}\")\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported start time type: {type(start_time)}\")\n",
        "\n",
        "    def _extract_timestamp_from_filename(self, filename_or_path):\n",
        "        \"\"\"Extract timestamp from video filename like '1735724709230.mp4'\"\"\"\n",
        "        filename = os.path.basename(filename_or_path)\n",
        "        name_without_ext = os.path.splitext(filename)[0]\n",
        "\n",
        "        # Look for 13-digit timestamp (milliseconds) or 10-digit (seconds)\n",
        "        timestamp_match = re.search(r'\\b(\\d{10,13})\\b', name_without_ext)\n",
        "\n",
        "        if timestamp_match:\n",
        "            timestamp_str = timestamp_match.group(1)\n",
        "            timestamp_num = int(timestamp_str)\n",
        "\n",
        "            if len(timestamp_str) == 13:  # Milliseconds\n",
        "                print(f\"📅 Found millisecond timestamp in filename: {timestamp_num}\")\n",
        "                return timestamp_num\n",
        "            elif len(timestamp_str) == 10:  # Seconds\n",
        "                print(f\"📅 Found second timestamp in filename: {timestamp_num}\")\n",
        "                return timestamp_num * 1000  # Convert to milliseconds\n",
        "\n",
        "        return None\n",
        "\n",
        "    def get_frame_timestamp_ms(self, frame_idx):\n",
        "        \"\"\"\n",
        "        Get the actual timestamp when this frame was captured in the video (in milliseconds)\n",
        "\n",
        "        Args:\n",
        "            frame_idx: Frame index in the video (0-based)\n",
        "        \"\"\"\n",
        "        # Calculate frame offset in milliseconds\n",
        "        frame_offset_ms = int((frame_idx / self.fps) * 1000)\n",
        "\n",
        "        # Add offset to video start time\n",
        "        return self.video_start_time_ms + frame_offset_ms\n",
        "\n",
        "    def get_formatted_timestamp(self, timestamp_ms):\n",
        "        \"\"\"Convert millisecond timestamp to readable format\"\"\"\n",
        "        dt = datetime.fromtimestamp(timestamp_ms / 1000.0)\n",
        "        return dt.isoformat()\n",
        "\n",
        "    def get_frame_info(self, frame_idx):\n",
        "        \"\"\"Get comprehensive frame timing information\"\"\"\n",
        "        frame_timestamp_ms = self.get_frame_timestamp_ms(frame_idx)\n",
        "\n",
        "        return {\n",
        "            'frame_idx': frame_idx,\n",
        "            'frame_timestamp_ms': frame_timestamp_ms,\n",
        "            'frame_timestamp_iso': self.get_formatted_timestamp(frame_timestamp_ms),\n",
        "            'video_start_time_ms': self.video_start_time_ms,\n",
        "            'video_start_time_iso': self.video_start_datetime.isoformat(),\n",
        "            'frame_offset_seconds': frame_idx / self.fps,\n",
        "            'frame_offset_ms': int((frame_idx / self.fps) * 1000),\n",
        "            'fps': self.fps\n",
        "        }\n",
        "\n",
        "\n",
        "class PlantNetIdentifier:\n",
        "    \"\"\"Handles PlantNet API v2 interactions for plant identification.\"\"\"\n",
        "\n",
        "    def __init__(self, api_key, project=\"all\"):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            api_key (str): Your PlantNet API key.\n",
        "            project (str): One of: \"all\", \"weurope\", \"canada\", \"australia\"\n",
        "        \"\"\"\n",
        "        self.api_key = api_key\n",
        "        self.project = project\n",
        "        self.base_url = f\"https://my-api.plantnet.org/v2/identify/{project}\"\n",
        "\n",
        "        if not api_key or api_key == \"YOUR_API_KEY\":\n",
        "            print(\"⚠️ WARNING: Please set your actual PlantNet API key!\")\n",
        "            print(\"Get your API key from: https://my.plantnet.org/\")\n",
        "\n",
        "    def crop_image_with_mask(self, image, mask, padding=20):\n",
        "        \"\"\"Crop a region from the image using the binary mask.\"\"\"\n",
        "        coords = np.column_stack(np.where(mask > 0))\n",
        "        if len(coords) == 0:\n",
        "            print(\"⚠️ No valid mask found.\")\n",
        "            return None\n",
        "\n",
        "        y_min, x_min = coords.min(axis=0)\n",
        "        y_max, x_max = coords.max(axis=0)\n",
        "        h, w = image.shape[:2]\n",
        "\n",
        "        y_min = max(0, y_min - padding)\n",
        "        y_max = min(h, y_max + padding)\n",
        "        x_min = max(0, x_min - padding)\n",
        "        x_max = min(w, x_max + padding)\n",
        "\n",
        "        cropped = image[y_min:y_max, x_min:x_max]\n",
        "        if cropped.size == 0:\n",
        "            print(\"⚠️ Cropped image is empty.\")\n",
        "            return None\n",
        "        return cropped\n",
        "\n",
        "    def crop_image_with_bbox(self, image, bbox, padding=20):\n",
        "        \"\"\"Crop image using bounding box coordinates.\"\"\"\n",
        "        x1, y1, x2, y2 = map(int, bbox)\n",
        "        h, w = image.shape[:2]\n",
        "\n",
        "        x1 = max(0, x1 - padding)\n",
        "        y1 = max(0, y1 - padding)\n",
        "        x2 = min(w, x2 + padding)\n",
        "        y2 = min(h, y2 + padding)\n",
        "\n",
        "        cropped = image[y1:y2, x1:x2]\n",
        "        if cropped.size == 0:\n",
        "            print(\"⚠️ Cropped image is empty.\")\n",
        "            return None\n",
        "        return cropped\n",
        "\n",
        "    def identify_plant(self, image_array, confidence_threshold=0.1):\n",
        "        \"\"\"Identify plant species in the given image using PlantNet API.\"\"\"\n",
        "        try:\n",
        "            if isinstance(image_array, np.ndarray):\n",
        "                if len(image_array.shape) == 3 and image_array.shape[2] == 3:\n",
        "                    pil_image = Image.fromarray(cv2.cvtColor(image_array, cv2.COLOR_BGR2RGB))\n",
        "                else:\n",
        "                    pil_image = Image.fromarray(image_array[:, :, :3])\n",
        "            else:\n",
        "                pil_image = image_array\n",
        "\n",
        "            if pil_image.size[0] < 50 or pil_image.size[1] < 50:\n",
        "                print(\"⚠️ Image too small for identification (min size 50x50).\")\n",
        "                return None\n",
        "\n",
        "            img_buffer = io.BytesIO()\n",
        "            pil_image.save(img_buffer, format='JPEG', quality=95)\n",
        "            img_bytes = img_buffer.getvalue()\n",
        "\n",
        "            url = f\"{self.base_url}?api-key={self.api_key}\"\n",
        "            files = {'images': ('tree.jpg', img_bytes, 'image/jpeg')}\n",
        "            data = {'organs': 'leaf'}  # You can also try 'flower', 'fruit', 'bark'\n",
        "\n",
        "            print(\"🌿 Sending tree identification request to PlantNet...\")\n",
        "            response = requests.post(url, files=files, data=data, timeout=30)\n",
        "            print(f\"🌐 Response status: {response.status_code}\")\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                response_data = response.json()\n",
        "                results = response_data.get('results', [])\n",
        "\n",
        "                if not results:\n",
        "                    return {\n",
        "                        'species': 'No match found',\n",
        "                        'common_names': [],\n",
        "                        'confidence': 0,\n",
        "                        'family': 'Unknown',\n",
        "                        'genus': 'Unknown'\n",
        "                    }\n",
        "\n",
        "                filtered = [r for r in results if r.get('score', 0) >= confidence_threshold]\n",
        "                best_match = filtered[0] if filtered else results[0]\n",
        "                species_info = best_match.get('species', {})\n",
        "\n",
        "                result = {\n",
        "                    'species': species_info.get('scientificNameWithoutAuthor', 'Unknown'),\n",
        "                    'common_names': species_info.get('commonNames', []),\n",
        "                    'confidence': best_match.get('score', 0),\n",
        "                    'family': species_info.get('family', {}).get('scientificNameWithoutAuthor', 'Unknown'),\n",
        "                    'genus': species_info.get('genus', {}).get('scientificNameWithoutAuthor', 'Unknown')\n",
        "                }\n",
        "\n",
        "                if result['confidence'] < confidence_threshold:\n",
        "                    result['species'] += ' (low confidence)'\n",
        "\n",
        "                return result\n",
        "\n",
        "            elif response.status_code == 401:\n",
        "                print(\"❌ 401 Unauthorized - check your API key.\")\n",
        "            elif response.status_code == 429:\n",
        "                print(\"❌ 429 Too Many Requests - rate limit exceeded. Waiting...\")\n",
        "                time.sleep(5)  # Wait longer before retry\n",
        "                return {\n",
        "                    'species': 'Rate Limit Exceeded',\n",
        "                    'common_names': [],\n",
        "                    'confidence': 0,\n",
        "                    'family': 'Unknown',\n",
        "                    'genus': 'Unknown'\n",
        "                }\n",
        "            else:\n",
        "                print(f\"❌ Unexpected API error ({response.status_code}): {response.text}\")\n",
        "                return {\n",
        "                    'species': 'API Error',\n",
        "                    'common_names': [],\n",
        "                    'confidence': 0,\n",
        "                    'family': 'Unknown',\n",
        "                    'genus': 'Unknown'\n",
        "                }\n",
        "\n",
        "            return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error during plant identification: {str(e)}\")\n",
        "            return {\n",
        "                'species': 'Exception Error',\n",
        "                'common_names': [],\n",
        "                'confidence': 0,\n",
        "                'family': 'Unknown',\n",
        "                'genus': 'Unknown'\n",
        "            }\n",
        "\n",
        "\n",
        "class IntegratedTreeDetectionSystem:\n",
        "    \"\"\"Main class that integrates GroundedSAM2 with PlantNet API.\"\"\"\n",
        "\n",
        "    def __init__(self, plantnet_api_key, plantnet_project=\"all\", video_start_time=None, video_fps=30.0):\n",
        "        \"\"\"Initialize the integrated system.\"\"\"\n",
        "        self.plant_identifier = PlantNetIdentifier(plantnet_api_key, plantnet_project)\n",
        "        self.tree_results = {}  # Store all results\n",
        "\n",
        "        # Initialize video timestamp generator\n",
        "        self.video_timestamps = VideoTimestampGenerator(video_start_time, video_fps)\n",
        "\n",
        "        self.setup_models()\n",
        "\n",
        "    def setup_models(self):\n",
        "        \"\"\"Initialize SAM2 and GroundingDINO models.\"\"\"\n",
        "        # Environment settings\n",
        "        torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()\n",
        "\n",
        "        if torch.cuda.get_device_properties(0).major >= 8:\n",
        "            torch.backends.cuda.matmul.allow_tf32 = True\n",
        "            torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "        # Model initialization\n",
        "        sam2_checkpoint = \"./checkpoints/sam2.1_hiera_large.pt\"\n",
        "        model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        self.video_predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint)\n",
        "        sam2_image_model = build_sam2(model_cfg, sam2_checkpoint, device=self.device)\n",
        "        self.image_predictor = SAM2ImagePredictor(sam2_image_model)\n",
        "\n",
        "        # GroundingDINO model\n",
        "        model_id = \"IDEA-Research/grounding-dino-tiny\"\n",
        "        self.processor = AutoProcessor.from_pretrained(model_id)\n",
        "        self.grounding_model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(self.device)\n",
        "\n",
        "    def process_video_with_species_identification(self, video_dir, output_dir, text_prompt=\"tree.\",\n",
        "                                                step=20, confidence_threshold=0.1, video_start_time=None, video_fps=30.0):\n",
        "        \"\"\"\n",
        "        Main pipeline: detect trees and identify species.\n",
        "\n",
        "        Args:\n",
        "            video_dir: Directory containing video frames\n",
        "            output_dir: Output directory for results\n",
        "            text_prompt: Text prompt for detection (default: \"tree.\")\n",
        "            step: Frame sampling step\n",
        "            confidence_threshold: Minimum confidence for PlantNet results\n",
        "            video_start_time: When the video was originally recorded (optional if set in constructor)\n",
        "            video_fps: Frames per second of the original video (optional if set in constructor)\n",
        "        \"\"\"\n",
        "\n",
        "        # Update video timestamps if provided\n",
        "        if video_start_time is not None:\n",
        "            self.video_timestamps = VideoTimestampGenerator(video_start_time, video_fps)\n",
        "\n",
        "        print(\"🚀 Starting integrated tree detection and species identification...\")\n",
        "        print(f\"📹 Video timing: {self.video_timestamps.video_start_datetime.isoformat()} @ {self.video_timestamps.fps} fps\")\n",
        "        print(f\"📹 Video start timestamp: {self.video_timestamps.video_start_time_ms} ms\")\n",
        "\n",
        "        # Setup directories\n",
        "        CommonUtils.creat_dirs(output_dir)\n",
        "        mask_data_dir = os.path.join(output_dir, \"mask_data\")\n",
        "        json_data_dir = os.path.join(output_dir, \"json_data\")\n",
        "        result_dir = os.path.join(output_dir, \"result\")\n",
        "        species_data_dir = os.path.join(output_dir, \"species_data\")\n",
        "\n",
        "        for dir_path in [mask_data_dir, json_data_dir, result_dir, species_data_dir]:\n",
        "            CommonUtils.creat_dirs(dir_path)\n",
        "\n",
        "        # Get frame names\n",
        "        frame_names = [\n",
        "            p for p in os.listdir(video_dir)\n",
        "            if os.path.splitext(p)[-1].lower() in [\".jpg\", \".jpeg\", \".png\"]\n",
        "        ]\n",
        "        frame_names.sort(key=lambda p: int(os.path.splitext(p)[0]))\n",
        "\n",
        "        # Initialize video predictor\n",
        "        inference_state = self.video_predictor.init_state(\n",
        "            video_path=video_dir,\n",
        "            offload_video_to_cpu=True,\n",
        "            async_loading_frames=True\n",
        "        )\n",
        "\n",
        "        sam2_masks = MaskDictionaryModel()\n",
        "        objects_count = 0\n",
        "\n",
        "        print(f\"📊 Total frames: {len(frame_names)}\")\n",
        "\n",
        "        # Process frames\n",
        "        for start_frame_idx in range(0, len(frame_names), step):\n",
        "            print(f\"🔍 Processing frame {start_frame_idx}...\")\n",
        "\n",
        "            # Load and process image\n",
        "            img_path = os.path.join(video_dir, frame_names[start_frame_idx])\n",
        "            image = Image.open(img_path)\n",
        "            image_base_name = frame_names[start_frame_idx].split(\".\")[0]\n",
        "\n",
        "            # Run GroundingDINO detection\n",
        "            detection_results = self._detect_trees(image, text_prompt)\n",
        "\n",
        "            if detection_results is None:\n",
        "                print(f\"⚠️ No trees detected in frame {start_frame_idx}\")\n",
        "                continue\n",
        "\n",
        "            # Get masks with SAM2\n",
        "            masks, boxes, labels = self._get_masks(image, detection_results)\n",
        "\n",
        "            if masks is None:\n",
        "                print(f\"⚠️ No masks generated for frame {start_frame_idx}\")\n",
        "                continue\n",
        "\n",
        "            # Identify species for each detected tree\n",
        "            species_results = self._identify_species_for_trees(\n",
        "                np.array(image), masks, boxes, labels, start_frame_idx, confidence_threshold\n",
        "            )\n",
        "\n",
        "            # Process video tracking\n",
        "            mask_dict = MaskDictionaryModel(\n",
        "                promote_type=\"mask\",\n",
        "                mask_name=f\"mask_{image_base_name}.npy\"\n",
        "            )\n",
        "            mask_dict.add_new_frame_annotation(\n",
        "                mask_list=torch.tensor(masks).to(self.device),\n",
        "                box_list=torch.tensor(boxes),\n",
        "                label_list=labels\n",
        "            )\n",
        "\n",
        "            objects_count = mask_dict.update_masks(\n",
        "                tracking_annotation_dict=sam2_masks,\n",
        "                iou_threshold=0.8,\n",
        "                objects_count=objects_count\n",
        "            )\n",
        "\n",
        "            # Propagate tracking\n",
        "            video_segments = self._propagate_tracking(\n",
        "                inference_state, mask_dict, start_frame_idx, step, frame_names\n",
        "            )\n",
        "\n",
        "            # Save results with species information\n",
        "            self._save_results_with_species(\n",
        "                video_segments, mask_data_dir, json_data_dir, species_results, species_data_dir\n",
        "            )\n",
        "\n",
        "        # Generate final visualization and video\n",
        "        self._create_final_output(video_dir, mask_data_dir, json_data_dir, result_dir, output_dir)\n",
        "\n",
        "        # Save comprehensive results\n",
        "        self._save_comprehensive_results(species_data_dir)\n",
        "\n",
        "        print(\"✅ Processing complete!\")\n",
        "\n",
        "    def _detect_trees(self, image, text_prompt):\n",
        "        \"\"\"Detect trees using GroundingDINO.\"\"\"\n",
        "        inputs = self.processor(images=image, text=text_prompt, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.grounding_model(**inputs)\n",
        "\n",
        "        results = self.processor.post_process_grounded_object_detection(\n",
        "            outputs,\n",
        "            inputs.input_ids,\n",
        "            box_threshold=0.25,\n",
        "            text_threshold=0.25,\n",
        "            target_sizes=[image.size[::-1]]\n",
        "        )\n",
        "\n",
        "        if len(results[0][\"boxes\"]) == 0:\n",
        "            return None\n",
        "\n",
        "        return results[0]\n",
        "\n",
        "    def _get_masks(self, image, detection_results):\n",
        "        \"\"\"Generate masks using SAM2.\"\"\"\n",
        "        self.image_predictor.set_image(np.array(image.convert(\"RGB\")))\n",
        "\n",
        "        input_boxes = detection_results[\"boxes\"]\n",
        "        labels = detection_results[\"labels\"]\n",
        "\n",
        "        masks, scores, logits = self.image_predictor.predict(\n",
        "            point_coords=None,\n",
        "            point_labels=None,\n",
        "            box=input_boxes,\n",
        "            multimask_output=False,\n",
        "        )\n",
        "\n",
        "        # Convert mask shape to (n, H, W)\n",
        "        if masks.ndim == 2:\n",
        "            masks = masks[None]\n",
        "        elif masks.ndim == 4:\n",
        "            masks = masks.squeeze(1)\n",
        "\n",
        "        return masks, input_boxes, labels\n",
        "\n",
        "    def _identify_species_for_trees(self, image, masks, boxes, labels, frame_idx, confidence_threshold):\n",
        "        \"\"\"Identify species for each detected tree.\"\"\"\n",
        "        species_results = {}\n",
        "\n",
        "        for i, (mask, box, label) in enumerate(zip(masks, boxes, labels)):\n",
        "            print(f\"🌲 Identifying species for tree {i+1} in frame {frame_idx}...\")\n",
        "\n",
        "            # Crop tree region using bounding box\n",
        "            cropped_tree = self.plant_identifier.crop_image_with_bbox(image, box, padding=30)\n",
        "\n",
        "            if cropped_tree is not None:\n",
        "                # Add small delay to respect API rate limits\n",
        "                time.sleep(0.5)\n",
        "\n",
        "                # Identify species\n",
        "                species_info = self.plant_identifier.identify_plant(\n",
        "                    cropped_tree, confidence_threshold=confidence_threshold\n",
        "                )\n",
        "\n",
        "                tree_id = f\"tree_{frame_idx}_{i}\"\n",
        "\n",
        "                # Get frame timing info based on video start time (in milliseconds)\n",
        "                frame_info = self.video_timestamps.get_frame_info(frame_idx)\n",
        "\n",
        "                species_results[tree_id] = {\n",
        "                    **frame_info,  # Include all timing information\n",
        "                    'tree_index': i,\n",
        "                    'bounding_box': box.tolist(),\n",
        "                    'detection_label': label,\n",
        "                    'species_identification': species_info if species_info is not None else {\n",
        "                        'species': 'Identification Failed',\n",
        "                        'common_names': [],\n",
        "                        'confidence': 0,\n",
        "                        'family': 'Unknown',\n",
        "                        'genus': 'Unknown'\n",
        "                    }\n",
        "                }\n",
        "\n",
        "                if species_info:\n",
        "                    print(f\"✅ Identified: {species_info['species']} (confidence: {species_info['confidence']:.2f})\")\n",
        "                    print(f\"🕐 Frame captured at: {frame_info['frame_timestamp_iso']} ({frame_info['frame_timestamp_ms']} ms)\")\n",
        "                else:\n",
        "                    print(\"❌ Species identification failed\")\n",
        "            else:\n",
        "                print(\"⚠️ Failed to crop tree region\")\n",
        "                tree_id = f\"tree_{frame_idx}_{i}\"\n",
        "\n",
        "                # Get frame timing info even for failed crops\n",
        "                frame_info = self.video_timestamps.get_frame_info(frame_idx)\n",
        "\n",
        "                species_results[tree_id] = {\n",
        "                    **frame_info,\n",
        "                    'tree_index': i,\n",
        "                    'bounding_box': box.tolist(),\n",
        "                    'detection_label': label,\n",
        "                    'species_identification': {\n",
        "                        'species': 'Crop Failed',\n",
        "                        'common_names': [],\n",
        "                        'confidence': 0,\n",
        "                        'family': 'Unknown',\n",
        "                        'genus': 'Unknown'\n",
        "                    }\n",
        "                }\n",
        "\n",
        "        return species_results\n",
        "\n",
        "    def _propagate_tracking(self, inference_state, mask_dict, start_frame_idx, step, frame_names):\n",
        "        \"\"\"Propagate tracking across frames.\"\"\"\n",
        "        self.video_predictor.reset_state(inference_state)\n",
        "\n",
        "        for object_id, object_info in mask_dict.labels.items():\n",
        "            frame_idx, out_obj_ids, out_mask_logits = self.video_predictor.add_new_mask(\n",
        "                inference_state,\n",
        "                start_frame_idx,\n",
        "                object_id,\n",
        "                object_info.mask,\n",
        "            )\n",
        "\n",
        "        video_segments = {}\n",
        "        for out_frame_idx, out_obj_ids, out_mask_logits in self.video_predictor.propagate_in_video(\n",
        "            inference_state, max_frame_num_to_track=step, start_frame_idx=start_frame_idx\n",
        "        ):\n",
        "            frame_masks = MaskDictionaryModel()\n",
        "\n",
        "            for i, out_obj_id in enumerate(out_obj_ids):\n",
        "                out_mask = (out_mask_logits[i] > 0.0)\n",
        "                object_info = ObjectInfo(\n",
        "                    instance_id=out_obj_id,\n",
        "                    mask=out_mask[0],\n",
        "                    class_name=mask_dict.get_target_class_name(out_obj_id)\n",
        "                )\n",
        "                object_info.update_box()\n",
        "                frame_masks.labels[out_obj_id] = object_info\n",
        "\n",
        "                image_base_name = frame_names[out_frame_idx].split(\".\")[0]\n",
        "                frame_masks.mask_name = f\"mask_{image_base_name}.npy\"\n",
        "                frame_masks.mask_height = out_mask.shape[-2]\n",
        "                frame_masks.mask_width = out_mask.shape[-1]\n",
        "\n",
        "            video_segments[out_frame_idx] = frame_masks\n",
        "\n",
        "        return video_segments\n",
        "\n",
        "    def _save_results_with_species(self, video_segments, mask_data_dir, json_data_dir, species_results, species_data_dir):\n",
        "        \"\"\"Save tracking results along with species information.\"\"\"\n",
        "        for frame_idx, frame_masks_info in video_segments.items():\n",
        "            # Save mask data\n",
        "            mask = frame_masks_info.labels\n",
        "            mask_img = torch.zeros(frame_masks_info.mask_height, frame_masks_info.mask_width)\n",
        "\n",
        "            for obj_id, obj_info in mask.items():\n",
        "                mask_img[obj_info.mask == True] = obj_id\n",
        "\n",
        "            mask_img = mask_img.numpy().astype(np.uint16)\n",
        "            np.save(os.path.join(mask_data_dir, frame_masks_info.mask_name), mask_img)\n",
        "\n",
        "            # Save JSON data with species information\n",
        "            json_data = copy.deepcopy(frame_masks_info.to_dict())\n",
        "\n",
        "            # Add species information if available\n",
        "            frame_species_info = {}\n",
        "            for tree_id, species_data in species_results.items():\n",
        "                if species_data['frame_idx'] == frame_idx:\n",
        "                    frame_species_info[tree_id] = species_data\n",
        "\n",
        "            json_data['species_identifications'] = frame_species_info\n",
        "\n",
        "            # Add video-based timing information (in milliseconds)\n",
        "            frame_info = self.video_timestamps.get_frame_info(frame_idx)\n",
        "            json_data.update(frame_info)\n",
        "\n",
        "            print(f\"[✔] Frame {frame_idx} timestamped as: {frame_info['frame_timestamp_ms']} ms ({frame_info['frame_timestamp_iso']})\")\n",
        "\n",
        "            json_data_path = os.path.join(json_data_dir, frame_masks_info.mask_name.replace(\".npy\", \".json\"))\n",
        "            with open(json_data_path, \"w\") as f:\n",
        "                json.dump(json_data, f, indent=2)\n",
        "\n",
        "        # Save species results separately\n",
        "        if species_results:\n",
        "            species_file = os.path.join(species_data_dir, f\"species_frame_{min(species_results.values(), key=lambda x: x['frame_idx'])['frame_idx']}.json\")\n",
        "            with open(species_file, \"w\") as f:\n",
        "                json.dump(species_results, f, indent=2)\n",
        "\n",
        "            # Update global results\n",
        "            self.tree_results.update(species_results)\n",
        "\n",
        "    def _create_final_output(self, video_dir, mask_data_dir, json_data_dir, result_dir, output_dir):\n",
        "        \"\"\"Create final visualization and video.\"\"\"\n",
        "        # Draw results\n",
        "        CommonUtils.draw_masks_and_box_with_supervision(video_dir, mask_data_dir, json_data_dir, result_dir)\n",
        "\n",
        "        # Create output video\n",
        "        output_video_path = os.path.join(output_dir, \"tree_detection_with_species.mp4\")\n",
        "        create_video_from_images(result_dir, output_video_path, frame_rate=30)\n",
        "        print(f\"📹 Output video saved: {output_video_path}\")\n",
        "\n",
        "    def _save_comprehensive_results(self, species_data_dir):\n",
        "        \"\"\"Save comprehensive results summary.\"\"\"\n",
        "        summary_file = os.path.join(species_data_dir, \"comprehensive_tree_analysis.json\")\n",
        "\n",
        "        # Create summary statistics\n",
        "        species_count = {}\n",
        "        total_trees = len(self.tree_results)\n",
        "\n",
        "        for tree_id, tree_data in self.tree_results.items():\n",
        "            species_info = tree_data.get('species_identification', {})\n",
        "\n",
        "            # Handle None species_info\n",
        "            if species_info is None:\n",
        "                species_name = 'Identification Failed'\n",
        "            else:\n",
        "                species_name = species_info.get('species', 'Unknown')\n",
        "\n",
        "            if species_name not in species_count:\n",
        "                species_count[species_name] = 0\n",
        "            species_count[species_name] += 1\n",
        "\n",
        "        summary = {\n",
        "            'analysis_summary': {\n",
        "                'total_trees_detected': total_trees,\n",
        "                'unique_species_found': len(species_count),\n",
        "                'species_distribution': species_count,\n",
        "                'analysis_timestamp_ms': int(time.time() * 1000),\n",
        "                'analysis_timestamp_iso': datetime.now().isoformat(),\n",
        "                'video_start_time_ms': self.video_timestamps.video_start_time_ms,\n",
        "                'video_start_time_iso': self.video_timestamps.video_start_datetime.isoformat(),\n",
        "                'video_fps': self.video_timestamps.fps\n",
        "            },\n",
        "            'detailed_results': self.tree_results\n",
        "        }\n",
        "\n",
        "        with open(summary_file, \"w\") as f:\n",
        "            json.dump(summary, f, indent=2)\n",
        "\n",
        "        print(f\"📊 Comprehensive results saved: {summary_file}\")\n",
        "        print(f\"🌳 Total trees detected: {total_trees}\")\n",
        "        print(f\"🌿 Unique species found: {len(species_count)}\")\n",
        "        for species, count in species_count.items():\n",
        "            print(f\"   - {species}: {count} trees\")\n",
        "\n",
        "\n",
        "# Usage example\n",
        "if __name__ == \"__main__\":\n",
        "    # Configuration\n",
        "    PLANTNET_API_KEY = \"Your_API_Key\"\n",
        "    PLANTNET_PROJECT = \"all\"  # or \"weurope\", \"canada\", \"australia\"\n",
        "\n",
        "    # Video timing information - USE YOUR ACTUAL VIDEO FILENAME TIMESTAMP\n",
        "    VIDEO_START_TIME_MS = 1751270034249 # Your video filename timestamp in milliseconds\n",
        "    VIDEO_FPS = 30.0  # Frames per second of your video\n",
        "\n",
        "    # Alternative ways to specify video start time:\n",
        "    # VIDEO_START_TIME_MS = \"1735724709230\"  # As string\n",
        "    # VIDEO_START_TIME_MS = \"1735724709230.mp4\"  # As filename (will extract timestamp)\n",
        "    # VIDEO_START_TIME_MS = datetime(2025, 1, 1, 14, 45, 9, 230000)  # datetime object\n",
        "    # VIDEO_START_TIME_MS = \"2025-01-01T14:45:09.230\"  # ISO format\n",
        "    # VIDEO_START_TIME_MS = None  # Will prompt for input\n",
        "\n",
        "    # Initialize the integrated system with video timing\n",
        "    tree_system = IntegratedTreeDetectionSystem(\n",
        "        PLANTNET_API_KEY,\n",
        "        PLANTNET_PROJECT,\n",
        "        video_start_time=VIDEO_START_TIME_MS,\n",
        "        video_fps=VIDEO_FPS\n",
        "    )\n",
        "\n",
        "    # Process video with tree detection and species identification\n",
        "    video_dir = \"frames/\"\n",
        "    output_dir = \"./outputs_with_species\"\n",
        "\n",
        "    tree_system.process_video_with_species_identification(\n",
        "        video_dir=video_dir,\n",
        "        output_dir=output_dir,\n",
        "        text_prompt=\"tree.\",\n",
        "        step=100,\n",
        "        confidence_threshold=0.1\n",
        "        # video_start_time and video_fps are optional here if already set in constructor\n",
        "    )"
      ],
      "metadata": {
        "id": "glBxM8eWepGw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}